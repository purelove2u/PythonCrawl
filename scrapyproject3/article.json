[
{"contents": "We are excited to announce our newest data extraction API. The  is now out of BETA and publicly available as a stable release.\u00a0If you are ready to roll up your sleeves and get started, here are the links you need:While this blog covers most of the notable improvements & extensive testing that the API has undergone, that warrants an exit from Beta, together with some high-level uses; it\u2019s important to remember that we have already covered it .We are moving AutoExtract Job Postings out of beta after making substantial , completely eliminating several classes of errors, and making . Aggregator websites where the API had a tendency to return failed requests on the BETA release have now been addressed, paving the way for widespread use.These changes were released to production as part of 20.5.0If you are looking to discern insights on the activities of organisations of all sizes, from start-ups to Fortune 100 companies, job postings can provide context for analysts to understand the market landscape. Where and how are competitors, suppliers and customers or even the industry, in general, structuring their business. Which technologies they are investing in, which ones they are no longer actively pursuing, what key markets are they pushing into, amongst other things.The technology stack of a start-up can (and should) look extremely different from that of a Fortune 1000 company. As organisations grow and evolve, expanding their workforce is a must to answer the on-going demands of the marketplace proactively. This is true to all sectors and industries but especially so in the context of the information technology industry, with so many roles and disciplines that need to be filled amid an ever-changing landscape.Imagine this scenario - an aspiring organisation within the Information Technology (IT) industry wants to expand into new markets. To do this, they need to recruit for a plethora of roles. From hiring cybersecurity professionals to either provide InfoSec support or man the Security Operations Centres (SOC), to\u00a0 DevOps/IT teams will be needed to deploy & maintain what the software engineers have developed; someone will have to project manage and someone needs to do the administrative heavy lifting.\u00a0IT is a competitive market to be in, and surely enough competitors are racing to have a head start. Within this context, business intelligence is paramount to gauge whether our fictitious organisations\u2019 plans are worthwhile in the first place or not. To accomplish this, their insights function needs to understand the recruitment practices of their top 10 competitors that operate a global workforce and have job listings in 100 different countries and 10 different languages - that is 10 x 100 = 1K websites to constantly monitor. With traditional manual scraping techniques, this will amount to a multi-man year-long project.\u00a0 With Scrapinghub\u2019s , a steady always-on reliable steam of data into our organisation\u2019s data warehouse and BI platform can be set up in a matter of days/hours/minutes.Our AutoExtract Job Postings API is tailor-made to answer the demands of the recruitment industry, particularly if data synchronisation between job boards or job aggregators and the individual recruitment agencies databases where uniformity, accuracy, semantic consistency and  reliability is an operational must.Without our Job Postings API, you would need to write custom code for each job posting page to extract and parse the data. On top of that, you would also have the maintenance overhead and all the troubleshooting that comes with it.\u00a0We are continuously improving the underlying machine learning technology, so you can be assured you get the highest quality job data possible.Using the API is simple:Our Job Posting data API is ideal forSome of the data fields you get in your API:Here\u2019s what you need to do if you want to get access to Job Postings API:If you want to check out any of our other AutoExtract APIs, PS: I would appreciate it if you let us know what you think of our new API in the comments."},
{"contents": "Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where (HCF) comes to our rescue.HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to , but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with , you may be wondering why one would use HCF, when Scrapy can store and recover the crawling state by itself.\u00a0The advantage is that Scrapy requires you to manage this state, by saving the content to disk (so needs disk quota) and if you are running inside a container, like in Scrapy Cloud, local files are lost once the process is finished. So, having some kind of external storage for requests is an alternative that takes this burden from your shoulders, leaving you to think about the extraction logic and not about the details on how to proceed in case it crashes and you need to restart.Before digging into an example of how to use HCF, I\u2019ll go over a bit on how it is structured. We can create many Frontiers per project, for each one we need a name. These Frontiers are then broken into slots, something similar to sharding, that can be useful in a producer-consumer scenario (topic of one of our upcoming blog posts). Usually, the name will be the name of the spider, to avoid any confusion. The catchy part is that we shouldn't change the number of slots after it was created, so keep it in mind when creating it.Now that we know what HCF is and how we could make use of it, it is time to see it working. For this purpose, we\u2019ll build a simple Scrapy spider to extract book information from . To get started, we\u2019ll create a new scrapy project and install the proper dependencies as shown below (type them in your terminal).The commands above will create a new directory for our project and create a new virtual environment, to avoid messing up our Operational System. Then it will install Scrapy and some libraries to use HCF. Finally, it creates a new Scrapy project and a spider. A side note on the extra libraries for HCF. There are a couple of libraries we could use, like , but it seems to be unmaintained for awhile. So, we\u2019ll be using  and HCF as a backed through .Given that our project was successfully created and the dependencies were installed, we can write a minimal spider to extract the book data as shown in the following code snippet.If you are familiar with Scrapy, there\u2019s nothing so fancy in the code above. Just a simple spider that navigates the book pages and follows book links to their pages to extract the title and price.We can run this spider from the terminal by typing Scrapy crawl books.toscrape.com and we should see the result there (no errors and 1,000 items were extracted). So far, we\u2019re not interacting with HCF and we\u2019ll be doing so by configuring it in the following changes. First, we\u2019ll need to update our project settings.py file with the following.The SCHEDULER, SPIDER_MIDDLEWARES and DOWNLOADER_MIDDLEWARES are set so scrapy-frontera works. Then, we set HCF as the BACKEND and add the proper Scrapy Cloud API Key (HCF_AUTH) and the project in which we\u2019re creating the Frontier (HCF_PROJECT_ID). With these settings in place, we can update our spider, so it starts interacting with HCF. If you run the spider now, you\u2019ll see some new logs, but it won\u2019t be storing the requests in HCF yet. The following changes should be applied in books_toscrape_com.py file.This is it, we\u2019ve got the moment that we can run our spider and it will be storing the requests in HCF. Just run the spider as we did before and it should work! But how can I tell that the requests were sent to HCF? For that, hcf-backend comes with a handy tool to help us, the hcfpal. From your terminal, just run the command below and you should see the Frontier name.There are some other commands available in hcfpal, like counting nthe requests in a given frontier.It will show you the request count per slot and total count (in case you have more than one slot).As we are storing the requests in HCF for further restart, it can be used as an example of incremental crawling. So, no need for special logic or so, just run the spider and it should start getting only new content. The requests are identified as in scrapy, by their fingerprint. There is one catch when working with multiple slots that is:, a given request is unique in a given slot (but we won\u2019t bother with it for now and leave it for a future article). To get started, let\u2019s clean our Frontier by typing the following in our terminal.Once it\u2019s done, run the spider but stop it before it finishes (simulating an early stop). To do it, press CTRL + C (Command + C) on the terminal once. It should send the signal to scrapy to finish the process. Then, wait a bit so the crawling process finishes. As the process finishes, it logs the stats in the terminal and we should use them to understand a bit of what\u2019s happening.For example, by looking into  I get that 80 items were extracted. Also, pay attention to stats starting with hcf/consumer and hcf/producer. These are related to the URLs we found in our run, how many were processed/extracted (consumed) and how many were discovered/stored (produced). In my case, it consumed 84 requests and found 105 links (all new, as we had cleaned the Frontier before running).After inspecting the stats, run the spider once again, without deleting the Frontier, and wait for it to finish. You should see that item_scrape_count is the difference between the previous crawl and the current one (in my case, 920 items). This happened because the duplicate requests were filtered by HCF and then they weren\u2019t processed again.You should also identify a similar behavior in  and  stats, showing that some links were extracted but not all of them are new.Finally,\u00a0 you can run the spider once more and it will just stop, logging no items scraped, because all the links it extracts were already processed in the previous runs. So, there is no new data to be processed and it finishes.HCF is a kind of external storage for requests that is available in Scrapy Cloud projects and it can be used by Scrapy spiders. There are many use cases for it, and we\u2019ve been through the recovery of a crawling process and incremental crawling scenarios. For a future article, we\u2019ll explore a bit more how we can configure HCF in our projects and how to use it in a producer-consumer architecture. If you got interested in it, I invite you to check the  (which has some information similar to this tutorial) and .If you want to learn more about web data extraction and how it can serve your business you can check out our  to see how others are making use of web data. Also, if you\u2019re considering outsourcing web scraping, you can watch our on-demand webinar to help you decide between  web data extraction."},
{"contents": "Article extraction is the process of extracting data fields from an article page and putting it into a machine-readable structured format like JSON. In many use cases, the article page that you want to extract is a news page but it can be any other type of article. Based on our experience in the web data extraction industry for over 10 years, the demand for structured article data is getting higher. There is more information available on the internet than ever. But still, having access to structured news data and being able to consume relevant and timely information can set you apart and give you a competitive edge. This is what article extraction can do for you.In order to extract article data from the web, you need an extraction tool. It can be a challenge to find the tool that is best suited to meet your needs and provides the functionality and data quality that you expect. In this article, we discuss the most used tools for article extraction that you can choose from.We\u2019re going to look at both open source and commercial solutions. Hopefully, by the end of this blog post, you will have a better understanding of the available article extraction tools and you will be able to make an educated decision whether an open-source or closed-source tool is the best for you to extract news and/or articles.Before looking at the tools, let\u2019s have a quick overview of what is included in article extraction and why you would extract articles.An article\u2019s most important data field and many times the reason you want to extract the data is the  field, which contains the text of the article. But other than the text body, there are many other fields you can and might want to extract.At Scrapinghub, we\u2019ve seen numerous web data extraction projects that included article extraction.As you can see, article data can provide the most value when building on top of it. But in order to achieve this, you need a reliable way to get structured article data feeds.An important requirement for any article extraction tool is that it needs to work for most websites without writing any site-specific code. The reason for this is that writing custom rule-based extraction code requires a lot of maintenance, especially if you\u2019re extracting data from hundreds or thousands of domains.Now let\u2019s look at the available tools!When searching for the best tool for any problem, you can be almost sure that there\u2019s an open-source library for that. That\u2019s the case for article extraction as well. Generally though, with open-source libraries, you always need to compromise on some functionality or quality compared to commercial solutions. Regarding OSS article extraction tools, there are quite a few libraries that you can choose from.is a library that extracts the main body text from the HTML and cleans it. This is a Python port of a Ruby port of arc90's readability project. It uses heuristics to determine which HTML elements belong to the article\u2019s body.is a Python 3 library that can extract and curate articles. It can also detect language automatically. It can extract a lot of fields from the article using its handy API.is Moz\u2019s open-source solution to extract articles. The library is based on machine learning models.is a Python wrapper around a Java library that removes boilerplate code and extracts text from HTML pages. It uses linguistically-motivated heuristics to determine the article body boundaries.extracts the full text from an HTML page. It does not try to separate the article body from the rest of the page, instead, giving all text present on the page.Generally, open-source article extraction solutions provide worse results than commercial ones. Even though for some use cases, they still can be good enough if unclean data is acceptable. If your goal is to just get all the content without missing anything, use or a similar \u201cdumb\u201d library. Keep in mind that simple HTML to clean text conversion is surprisingly tricky. Straightforward solutions like the Xpath string() function can produce messy whitespaces, missing or additional text.The go-to solution for extracting information from a web page is writing some custom code to do the job. A common choice is to write code in Python which then extracts information using XPath or CSS selectors, or sometimes traversing the HTML structure directly. This works well if you want to extract information from a single website with well-defined fields (e.g. product prices) where an XPath selector might look something like \u2018//div[@id=\u201dprice\u201d]\u2019 - that is, find a \u201cdiv\u201d element with \u201cid\u201d equal to \u201cprice\u201d.This works well, provided the website has such an element, with the main drawbacks being the time it takes to write and test manual extraction code, and the need to update the code when the website changes. Still, it\u2019s a fine approach in many cases, although it can quickly break when you\u2019re dealing with thousands of websites.But such an approach often does not work well even for a single website in the case of articles, not speaking of thousands of websites. Why is that? The reason is that even when we find the HTML element which contains the article text, extracting all text from this element often leads to poor results, as this element often contains a lot of unnecessary text, such as author information, advertisements, links to \u201crelated\u201d articles from the same platform, comments, tags, forms to subscribe, social network share buttons, etc. While some of these elements could be useful on their own, they are definitely not part of the clean article text and must be removed. But the rules for removal of such elements are quite hard to define, e.g. many of the above elements could be under just ordinary \u201cp\u201d (paragraph) or \u201cul\u201d (list) tags, and such rules would be often hard to scale from one page of the same website to another.Machine learning shines when such rules become too hard to maintain. With ML, instead of writing and maintaining thousands of rules, we create an annotated dataset, where we can specify the desired extraction result for each page in the dataset. Then the machine learning model is trained on the dataset, automatically deriving a set of features and weights that generalize well across different pages and websites. The resulting model can be applied to a new web-site without any tuning, providing good out-of-the-box quality, and allowing to collect articles from tens of thousands of sources. is Scrapinghub\u2019s automatic tool to extract structured articles. News API only needs the page URLs you want to extract the article from and then delivers the structured data. Main features include:Diffbot\u2019s Article API extracts clean text from news articles.Webhose turns unstructured web content into machine-readable data feeds.When it comes to article extraction there are some solutions that specialize in a specific niche. These solutions, going a step further from getting you access to structured data feeds, also provide some kind of  or analytics on top of the data. These providers usually specialize in the financial industry where relevant and timely information is key to success.Some considerations for niche-specific article extraction services:When it comes to web data extraction, data quality is always a key factor. At Scrapinghub, we continuously monitor the quality of AutoExtract News API extraction quality against benchmarks and other tools. We also created an . you can learn about what metrics are important when measuring article body quality and it might help you choose your article extraction tool.Article body  if your business depends on this kind of data. If you\u2019re developing a product or software that needs structured article/news data constantly, you need to make sure you choose a solution which has the best quality on the market. This is what our whitepaper helps you with.Commercial solutions do provide better quality, as you can see in our evaluation. Besides better quality, commercial services provide other features as well. For example, AutoExtract News API can get you cleaned and normalized HTML of the article, article author, headline, date posted, images and many other attributes. It can also handle downloading, which is a whole different aspect and significantly affects the final result.As discussed above, commercial solutions do provide better extraction quality, which is crucial for many use cases.Furthermore, a commercial service like  can also extract cleaned and normalized HTML of the article, including article author, headline, publishing date, images and many other attributes.Downloading HTML files can seem easy but in reality it has a lot of challenges like javascript rendering and proxy management. Commercial services take care of HTML downloading as well.Based on our research, it\u2019s safe to say that the quality of article extraction is significantly worse when using open source libraries. Even the most precise open source library provides 4.6x more unwanted content in the results while missing 2.5x more content than . That being said, open source libraries still can be useful in some use cases where unclean or messy data is acceptable to meet requirements."},
{"contents": "But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution. proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean.The biggest issue with residential proxies is, as it was mentioned, they are expensive. So usually the most effective way to scale your web data extraction project, is to try to maximize the value of data center proxies, by being smart about how you actually scrape the web and how you use proxies.Two things, that you can do to achieve this:If you want to learn more about these tactics, I recommend watching our FREE webinar on this: If you missed our webinar on the topic of data center proxies and residential proxies don\u2019t worry you will be able to watch it here:If you feel like you know enough already and you don't want to spend way too much time on managing proxies, you can just use an already existing solution for ."},
{"contents": "We are excited to announce our newest data extraction API. The  is now publicly available as a BETA release.If you want to skip the introductions and just get stuck in, here are the links you need:AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as  and more into the arena of blog comment analysis.\u00a0The underlying data model for the API was released to production as part of 20.6.0 release of AutoExtract.Customer support management presents many challenges due to the sheer number of requests, varied topics, and diverse departments within a company that might have a say in resolving the matter.Sourcing structured data from blog comments as provided by our API can be used in tandem with  solutions to quickly and effectively identify, track and act upon particular conversation strings \u2018hidden\u2019 amongst the noise of thousands of comments. You are effectively highlighting warning signs that your CX team should become involved before an incident takes place.Another particular powerful insight that can be derived from comments revolving around the sphere of Voice of Customer (VoC) and product analysis. By tapping into blog comments, you can search keywords for a particular product or feature or use the parsed data to train sentiment analysis model to find only the information you need.Without our Comments API (Beta), you would need to write custom code for each blog post to extract and parse the data. Let alone the time and overhead spend required to maintain the necessary infrastructure to deliver the data in real-time, in a scalable fashion and .Using the API is simple:Here\u2019s an example response: for more information about the fields.Here\u2019s what you need to do if you want to get access to Comments API:If you want to check out any of our other AutoExtract APIs, PS: I said it before, no better place to gain product feedback than in a blog post comment. Please share your thoughts\u00a0 in the comments below\ud83d\udc47\u00a0\ud83d\ude4f"},
{"contents": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions.However, as the number of websites continues to grow, data extraction has become increasingly complex. Constantly changing website structures, antibot practices and scaling requirements add to the difficulty.To overcome the ever-increasing challenges, the web data extraction industry has gone through impressive innovations and transformations in recent years that enable extracting accurate data in an efficient manner. During this revolutionary time in the industry, we are delighted to announce that Scrapinghub will be once again hosting the this year on .This year, however, things will be a little different. Given Inspired by the great success of some of the talks during the Web Data Extraction Summit 2019 and feedback from attendees, we are planning a  this year. Sessions will include talks ranging from basic approaches to scrape to advanced level use of AI and machine learning in data extraction. Along with use cases on how companies from different industries use web extracted data and follow-up sessions on some interesting topics covered last year such as tackling antibots. Our Data Scientists and Product Engineers will also give you a sneak peek of some of the most cutting edge technologies in the web data extraction industry.\u00a0Don\u2019t miss out on the chance to be amidst hundreds of data lovers and hear from some of the leaders and pioneers of the web scraping and data extraction industry.Oh, and did we mention it's completely free!\u00a0We will be announcing more talks are we get closer to , along with some exciting competitions very soon, so make sure you follow us on social media to stay updated!\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0"},
{"contents": "Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.During our webinar on \u201c\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code.A: Websites showcase erroneous pricing data when they detect you scraping regularly. This especially happens when you are trying to scale - i.e scrape a lot of products very frequently. Erroneous pricing is not easily recognizable, but comparing the prices or other data fields with previously extracted data and manually checking if there is a big difference in the extracted data can help.The long-term solution for this would be to be smarter about how you scale and be more thoughtful about the  you use.A: Scraping accurate data is all about having a reliable quality assurance process. The first step towards this process is to have a well-defined JSON schema. Your QA process needs to be a balanced combination of automated ways of testing the data as well as manual ways. This blog post gives a detailed description of .A: For javascript heavy sites, the simplest way would be to inspect the website and see if it uses any hidden APIs that have the data in JSON or other simple formats. This way, you can get the data without executing the javascript.However, sometimes that may not work. In that case, you will need to execute the javascript using a headless browser like  or Selenium or Puppeteer. The challenge here is that it will consume more resources making the process more expensive.A: There are many ways to conduct product matching. The main idea would be to gather as many product-specific parameters as you can about the product that you want to match and then compare those parameters.Eg: For a TV, the product-specific parameters would be resolution, weight, sound, etc. If in comparison, 90% of the parameters of any two products are the same, there is a high chance that it is the same product across two websites.You can build models on this concept to identify product duplication.Want to know more about how you can fuel your price intelligence decisions with web extracted data? Watch this webinar where our Technology Evangelist Attila Toth takes you on a deep dive through the main challenges affecting price intelligence projects from both a business and technical perspective, and more importantly, how you can solve them.If you have any more questions or queries on Price Intelligence data extraction, feel free to leave a comment below and we will try our best to answer them.\u00a0"},
{"contents": "Generally, there are 3 steps needed to find the best proxy management method for your web scraping project and to make sure you can get data not just today but also in the future, long-term.You need to define the traffic profile first to determine the concrete needs of your project. What is a traffic profile?It includes, first of all, the  that you're trying to get data from. And also if there's any technical challenges needed to be solved, like JS rendering.The traffic profile also includes the , meaning how many requests do you want to or need to make per hour or per day. Also do you have any specific time window for the requests, like, for example you want to make all your requests only during work hours, for some reason. Or is it okay to get the data at night, when there's significantly less traffic hitting the site.Then the last thing in the traffic profile is, . Because sometimes the website displays different content depending on where you are. So you need to use proxies that are in that specific region you need.So these three elements together make the traffic profile: websites, volume and geo locations. Now, with these, you can determine the exact proxy situation that you need a solution for.The next step to scale up is to get a proxy pool. Based on the traffic profile, now you can estimateYou can get access to proxies directly from proxy providers, or through a proxy management solution as well. The drawback of getting proxies directly from providers -and not through a management solution - is that you need to do the managing yourself. There are a lot of things you need to look out for if you go with a provider that doesn\u2019t provide management of proxies.The final step is . Because it's not enough to have just a proxy pool. You also need to use the proxies efficiently. For example, some features that our smart  network has to manage proxies and maximize their value:But either you're using Crawlera, or you create your own proxy management solution there are some key points to focus on if you want long-term scalability.First of all, . Because if you're extracting data at scale, most probably, you will not have issues with parsing HTML and writing the spider. But you WILL have issues with proxies. That's why it needs to be a priority.Then, if you are managing your own proxies, it's important to keep the proxy pool clean and healthy. If you use a proper management service, it's not a problem, as that handles it for you.Finally, my last point is to  to websites. Ultimately, it is a huge factor when scaling a web scraping project. You don't want to hit websites too hard and you need to make sure you follow the website's rules.But again, if you're using a management tool, you will have a much easier time with proxies because everything is taken care of under the hood, you just need to send requests and extract the data.If you want to learn more, we have webinars on the topic of  and also about how to , where we go into more details.And if you want to try Crawlera, the smart proxy network, you can do it for free.\u00a0"},
{"contents": "As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data.In this article, you will see the many ways real estate data can help you and how utilizing web scraping can help you and your organization become disruption-proofed and fully prepared for the world of tomorrow.There is more public data available in the real estate market than ever. There are numerous listing sites, endless data points available for everyone to see. And if there\u2019s data, there should be a way to  from the data to make better decisions. But there\u2019s one big problem...Unfortunately, many websites don\u2019t provide . Or even if they do, you might not get all the data you want only in a limited fashion. But still, the publicly available data is there, you just don\u2019t have a straightforward way to get the data. This is where web data extraction comes in. Web data extraction allows you to get this publicly available real estate data at scale. Using the correct tools or partnering with a , like Scrapinghub, allows you to tap into the world of web scraping and enjoy the benefits of .There are many situations where estimating the value of a property is necessary. Maybe you\u2019re trying to list it online for the most accurate price, maybe you\u2019re trying to get financing or you\u2019re analyzing a property before purchasing. You want to get the most accurate value of how much the property is worth.Being in the real estate market means that you have a lot of competition. In order to be ahead of the competition, you need to find ways to know more than others. As most realtors get their data from a single listing like the MLS, you can differentiate yourself by accessing alternative data sources. Web data extraction can help by allowing you to fetch structured  from any publicly available listing website. And as web scraping is a new technology for many, it can give you huge value as you will have considerably more data, and thus, information, in your hands.With web scraping, you can gather all the data points that exist about the given property, if it\u2019s available online. Then, you can use this data to justify your price or position your offer more accurately. Because you see the full picture through web data, you have a better chance to accurately estimate the value of a property.You may have heard this mantra by agents and realtors. Location is one of the key factors that determine the value of a property. Unfortunately, it\u2019s not straightforward to get access to from only that specific area you want to analyze.With web scraping though, you can automate the process of filtering through data so you only extract data that matters to you. Or you can just grab the whole market data from the web then filter the data yourself, depending on your requirements.When it comes to properties, there are many numeric data points that can influence the price: square footage, age, lot size, last sold price, etc. When buying a property for yourself, emotions play a huge role in your decision making. Sometimes you are willing to pay more because you have strong emotional reasoning.But also, it is always important to look at the raw numbers of a property before purchasing. You can make smarter - and more logical - decisions if you first look at the raw data and make a data-driven decision. Especially when it\u2019s a property you purchase for investment purposes. Without web scraping, you cannot see the full market\u2019s prices and other data points in a structured way.When buying a property for investment purposes, the vacancy rate is a crucial factor that can be a dealbreaker or even one of the main reasons you purchase a property. If the vacancy rate goes down in a market, the rents are expected to increase because the demand is higher. On the other hand, if the vacancy rate goes up that means the demand is lower so the rents are expected to decrease.Unfortunately, many agents use a static vacancy rate when analyzing a property and disregard the actual data. They do this, simply, because they don\u2019t have time to do the research themselves. Fortunately, with the help of web scraping, it doesn\u2019t take that much time to gather high-dimensional data about the real estate market and calculate the expected vacancy rate more accurately. Collecting fresh pricing and rents data, along with recent property completions and calculating lease lengths can help you to determine vacancy rates.The real estate market is always changing, going through cycles. The challenge is to identify where it\u2019s going right now and where it will be in the future.  is important to properly value property and to make investment decisions. These insights lie in the raw data of the real estate market. It would be impossible for an individual to gather all the data manually. That\u2019s why web data extraction can provide so much value by giving you all the data there is, in a timely manner.Also, if you start monitoring the real estate market today with the help of web scraping, in the upcoming months and years you will have a tremendous amount of historical data. Mining this data can help you see the direction the market is moving towards and show you patterns you wouldn\u2019t have been able to recognize otherwise.Scraping real estate data can seem simple at the beginning. These are the general steps if you decide to :You will need to extract a  if you want to get the most insights. For this, you need to  There are  you need to go through if you decide to extract data from the web at a large scale.Solving all these challenges takes a lot of resources and technical experience in web scraping. Unless web scraping is the core of your business (probably not), you might be better off partnering with a vendor that can solve these problems for you, so you only get the quality data but don\u2019t have to deal with the hurdles.Scraping real estate data has huge potential for anyone who is in the real estate market. Especially because for many this is still an untapped opportunity. Also, the web data extraction tools you can choose from have evolved a lot in the past years so either you do it yourself or partnering with a web  like Scrapinghub, you will be in a great position to start gaining value from public web data and get a competitive edge.If you want to learn more about how to make the most out of real estate data, download our whitepaper here: \u00a0"},
{"contents": "In this article, we build upon some of the semi-automated techniques and tools introduced in the  of the series.Let\u2019s say that the data we work with is separated by comma and line breaks:However, there isn\u2019t a consistency of how many words are separated by comma per line. To make it easier, we can transform the data set so there\u2019s only one word in each:In order to make this transformation of the data we can use search and replace functionalities of code text editors such as ,  or .\u00a0This how you can do it using Sublime Text:Once the process finishes, all the words will be in a single row, separated by commas:Finally, we replace commas with  - newlineOnce the replacing is done, we have a normalized dataset with only one word per line.Let's work again with data from . Our goal for this example is to make sure that the  stated on the website are indeed the top ten tags present in the scraped data. The web page looks like this:\u00a0After scraping the data from the page and loading it into a spreadsheet. this is what it looks like:We will be using Google Sheets for this example. The first step will be to split the tags column into several columns so that we can count each word individually and analyze the results better:Then your spreadsheet will look like this:The next step will be to convert the multiple tag columns into a single one. This can be done by defining a new range:Then apply the following formula to our data and expanded up to total expected length (in this case 8 columns by 100 words = 800 rows in total):=INDEX(Tags,1+INT((ROW(A1)-1)/COLUMNS(Tags)),MOD(ROW(A1)-1+COLUMNS(Tags),COLUMNS(Tags))+1)When you reach the end you will see an error \u201c#NUM!\u201d as shown below:\u00a0Next, we create a new column next to the tags populated with 1:You can enter 1 in cells N2 and N3 and then click twice on the bottom angle of the cell to duplicate the value to the rest of the rows.The final step is to create a Pivot table with columns M and N thus:\u00a0Then, this should be the result:Finally, we can sort our data by occurrence count - this can be done from the Pivot table editor which gives us the following:The top tags were verified in the previous example. What if we need to open the URLs of the top 10 and visually check them or random tags? In this case, we can use a combination of tools such as:If we take a closer look at the tag links, they follow this structure:Let\u2019s say we are picking the following tags:To visually check them, we can:The result will be:Now we can open all of them simultaneously by using the Chrome extension we mentioned, .After checking the results page by page comparing with the scraped data we can close the ones without a problem and keep the bad ones by:The final result you\u2019ll have for analysis or reporting issues will be:Whenever it\u2019s visible that side by side comparison could be something helpful, diff check tools (simple text comparison tools such as WinMerge) could be used. For this example, let\u2019s say we received the task to assure all category links of books from  were indeed scraped.The data would look similar to this:The website has a listing of the categories just like this:\u00a0So since it\u2019s pretty simple to select all the categories we can copy it and then simply side by side compare it with the extracted data.Firstly upon copying all the categories from the website, we will need to normalize it in a similar way the scraped category links will be as well. So let\u2019s order them alphabetically and transform to lower case.In order to transform everything to lowercase, I\u2019ll be using a built-in command called \u201cTransform to Lowercase\u201d\u00a0 for Visual Studio Code (through the command palette - open through F1 keyboard key):Then I\u2019ll use an extension for Visual Studio Code called  which upon using the available commands, will give us the following result that later will be compared to the category_links scraped data:Now we normalize the category_links from the data scraped removing everything else not related to the category name with search and replace approaches similar to what was shown in the first method:Removal of the start part of the URLs:Then removing the header and the final part:Now wrapping up with everything else (replacing \u201c-\u201d with one whitespace and removing a final leftover part):Then we go over to WinMerge, open a new window through CTRL + N, paste the copied content from the website in one side and the categories from the links we normalized in the other. Upon hitting F5, both will be compared and if equal, every single option shown were indeed scraped:Some scraping methods rely on data and structures that may be available only internally in the page source code like microdata and JSON linked data. Therefore, to make the job of assuring the data quality easier and compare with the scraped data, these can be checked with a tool called .SQL has been a key tool in the QA Engineer\u2019s toolbelt. Let\u2019s suppose we have 1000 rows of data scraped from  following the below pattern to assess its data quality:Apart from field names not matching nicely for the scraped data (like a \u201ctitle\u201d column actually having URLs instead), with SQL we can:\u00a0\u00a0However, let\u2019s suppose we expected the field \u201c_type\u201d to be the genre/category of the books:All data had only one value for \u201c_type\u201d and therefore could be corrected to scrape the genre of the books instead to be more useful.In this post, we showcased multiple semi-automated techniques which combined with the approaches shown in the previous posts of the series will, hopefully, help bring creative ideas into your data quality assurance process to test your data better.\u00a0Web scraping can look deceptively easy when you're\u00a0 starting out. There are numerous open-source libraries/frameworks and data extraction tools that make it very easy to scrape data from a website. But in reality it can be very hard to extract web data at scale. Read our whitepaper and learn "}
]